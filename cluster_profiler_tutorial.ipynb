{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Profiler Tutorial: Understanding What Makes Clusters Different\n",
    "\n",
    "This notebook demonstrates how to use the **Cluster Profiler** package to statistically analyze what features characterize different clusters in your data.\n",
    "\n",
    "## What is Cluster Profiling?\n",
    "\n",
    "After clustering your data, you often ask: *\"What makes each cluster unique?\"* \n",
    "\n",
    "Cluster Profiler answers this by:\n",
    "- Testing each feature for significant differences between each cluster vs. all others\n",
    "- Calculating effect sizes to measure the magnitude of differences\n",
    "- Correcting for multiple testing to avoid false discoveries\n",
    "- Ranking features by statistical importance, not just p-values\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from cluster_profiler import ClusterProfiler\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating a Realistic Test Dataset\n",
    "\n",
    "We'll create a dataset that mimics real-world scenarios where:\n",
    "1. **Some features truly differentiate clusters** (meaningful)\n",
    "2. **Some features are just noise** (random)\n",
    "3. **Features have different distributions** (normal, skewed, categorical)\n",
    "\n",
    "This helps us test if our profiler correctly identifies meaningful vs. random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Generate base clusters using make_blobs\n",
    "# Why make_blobs? It creates well-separated clusters in continuous space\n",
    "X, true_labels = make_blobs(n_samples=300, centers=3, n_features=4, \n",
    "                           cluster_std=1.5, random_state=42)\n",
    "\n",
    "# Create DataFrame with meaningful names\n",
    "data = pd.DataFrame(X, columns=['income', 'age', 'education_years', 'experience'])\n",
    "\n",
    "print(f\"Base dataset shape: {data.shape}\")\n",
    "print(f\"Number of natural clusters: {len(np.unique(true_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b: Add a categorical feature that correlates with clusters\n",
    "# This simulates real scenarios like \"customer type\" varying by cluster\n",
    "\n",
    "job_categories = []\n",
    "for label in true_labels:\n",
    "    if label == 0:  # Cluster 0: Mostly \"Tech\" workers\n",
    "        job_categories.append(np.random.choice(['Tech', 'Finance', 'Healthcare'], p=[0.7, 0.2, 0.1]))\n",
    "    elif label == 1:  # Cluster 1: Mostly \"Finance\" workers  \n",
    "        job_categories.append(np.random.choice(['Finance', 'Tech', 'Healthcare'], p=[0.6, 0.3, 0.1]))\n",
    "    else:  # Cluster 2: Mostly \"Healthcare\" workers\n",
    "        job_categories.append(np.random.choice(['Healthcare', 'Tech', 'Finance'], p=[0.8, 0.1, 0.1]))\n",
    "\n",
    "data['job_category'] = job_categories\n",
    "\n",
    "print(\"Job category distribution by true cluster:\")\n",
    "pd.crosstab(true_labels, data['job_category'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c: Add a skewed feature that varies by cluster\n",
    "# This simulates features like \"spending\", \"response time\", etc.\n",
    "\n",
    "spending_amounts = []\n",
    "for label in true_labels:\n",
    "    if label == 0:  # Low spenders\n",
    "        spending_amounts.append(np.random.exponential(500))  \n",
    "    elif label == 1:  # Medium spenders\n",
    "        spending_amounts.append(np.random.exponential(1500))  \n",
    "    else:  # High spenders\n",
    "        spending_amounts.append(np.random.exponential(3000))  \n",
    "\n",
    "data['monthly_spending'] = spending_amounts\n",
    "\n",
    "print(f\"Spending by cluster - Mean (Std):\")\n",
    "for i in range(3):\n",
    "    cluster_spending = data.loc[true_labels == i, 'monthly_spending']\n",
    "    print(f\"Cluster {i}: ${cluster_spending.mean():.0f} (${cluster_spending.std():.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1d: Add random features (should NOT be significant)\n",
    "# These test if our profiler correctly identifies noise\n",
    "\n",
    "data['random_category'] = np.random.choice(['A', 'B', 'C'], size=300)\n",
    "data['random_normal'] = np.random.normal(50, 10, 300)\n",
    "data['random_uniform'] = np.random.uniform(0, 100, 300)\n",
    "\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"- Continuous (meaningful): income, age, education_years, experience, monthly_spending\")\n",
    "print(f\"- Categorical (meaningful): job_category\")\n",
    "print(f\"- Random features: random_category, random_normal, random_uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualizing the Data\n",
    "\n",
    "Let's visualize our clusters and feature distributions to understand what we expect to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering (we'll use 3 clusters to match our true structure)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)  # Use only the original blob features\n",
    "\n",
    "# Add cluster labels to our dataframe\n",
    "data['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"Cluster sizes: {np.bincount(cluster_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: True clusters\n",
    "scatter1 = axes[0].scatter(data['income'], data['age'], c=true_labels, alpha=0.7)\n",
    "axes[0].set_title('True Clusters (from make_blobs)')\n",
    "axes[0].set_xlabel('Income')\n",
    "axes[0].set_ylabel('Age')\n",
    "plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "# Plot 2: KMeans clusters\n",
    "scatter2 = axes[1].scatter(data['income'], data['age'], c=cluster_labels, alpha=0.7)\n",
    "axes[1].set_title('KMeans Clusters (what we analyze)')\n",
    "axes[1].set_xlabel('Income')\n",
    "axes[1].set_ylabel('Age')\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by cluster\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Monthly spending (skewed, meaningful)\n",
    "for cluster in range(3):\n",
    "    cluster_data = data[data['cluster'] == cluster]['monthly_spending']\n",
    "    axes[0,0].hist(cluster_data, alpha=0.6, label=f'Cluster {cluster}', bins=20)\n",
    "axes[0,0].set_title('Monthly Spending Distribution (Meaningful + Skewed)')\n",
    "axes[0,0].set_xlabel('Spending ($)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Job category (categorical, meaningful)\n",
    "job_counts = pd.crosstab(data['cluster'], data['job_category'])\n",
    "job_counts.plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Job Category by Cluster (Meaningful)')\n",
    "axes[0,1].set_xlabel('Cluster')\n",
    "axes[0,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Random normal (should not differ by cluster)\n",
    "for cluster in range(3):\n",
    "    cluster_data = data[data['cluster'] == cluster]['random_normal']\n",
    "    axes[1,0].hist(cluster_data, alpha=0.6, label=f'Cluster {cluster}', bins=20)\n",
    "axes[1,0].set_title('Random Normal Distribution (Should NOT be meaningful)')\n",
    "axes[1,0].set_xlabel('Random Value')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Random category (should not differ by cluster)\n",
    "random_counts = pd.crosstab(data['cluster'], data['random_category'])\n",
    "random_counts.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Random Category by Cluster (Should NOT be meaningful)')\n",
    "axes[1,1].set_xlabel('Cluster')\n",
    "axes[1,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessing Feature Skewness\n",
    "\n",
    "Before profiling, let's check which features are skewed and might benefit from preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the profiler\n",
    "profiler = ClusterProfiler(alpha=0.05)\n",
    "\n",
    "# Assess skewness of continuous features\n",
    "skewness_report = profiler.assess_skewness(data)\n",
    "print(\"Skewness Assessment:\")\n",
    "print(\"=\" * 60)\n",
    "print(skewness_report[['feature', 'skewness', 'interpretation', 'recommend_preprocessing']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize skewness\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "continuous_features = ['income', 'age', 'education_years', 'experience', 'monthly_spending', 'random_normal']\n",
    "\n",
    "for i, feature in enumerate(continuous_features):\n",
    "    data[feature].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "    skew_val = skewness_report[skewness_report['feature'] == feature]['skewness'].iloc[0]\n",
    "    axes[i].set_title(f'{feature}\\nSkewness: {skew_val:.2f}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- monthly_spending is highly right-skewed (typical for spending data)\")\n",
    "print(\"- Other features are approximately symmetric\")\n",
    "print(\"- Preprocessing might help with monthly_spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Cluster Profiling - The Main Analysis\n",
    "\n",
    "Now let's profile our clusters to see which features characterize each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile clusters without preprocessing first\n",
    "print(\"CLUSTER PROFILING WITHOUT PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_raw = profiler.profile_clusters(data.drop('cluster', axis=1), cluster_labels)\n",
    "profiler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top results by importance score\n",
    "print(\"\\nTop 10 Features by Statistical Importance:\")\n",
    "print(\"=\" * 50)\n",
    "top_results = results_raw.nlargest(10, 'importance_score')\n",
    "display_cols = ['cluster', 'feature', 'effect_size', 'p_value_corrected', 'importance_score', 'significant_corrected']\n",
    "print(top_results[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with preprocessing for skewed features\n",
    "print(\"\\nCLUSTER PROFILING WITH YEO-JOHNSON PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "profiler_prep = ClusterProfiler(alpha=0.05, preprocessing='yeo-johnson')\n",
    "results_prep = profiler_prep.profile_clusters(data.drop('cluster', axis=1), cluster_labels)\n",
    "profiler_prep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare preprocessing effects\n",
    "print(\"\\nComparison: Effect of Preprocessing on monthly_spending\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "spending_raw = results_raw[results_raw['feature'] == 'monthly_spending']\n",
    "spending_prep = results_prep[results_prep['feature'] == 'monthly_spending']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Cluster': spending_raw['cluster'].values,\n",
    "    'Raw_Effect_Size': spending_raw['effect_size'].values,\n",
    "    'Raw_P_Value': spending_raw['p_value_corrected'].values,\n",
    "    'Preprocessed_Effect_Size': spending_prep['effect_size'].values,\n",
    "    'Preprocessed_P_Value': spending_prep['p_value_corrected'].values\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nNote: Preprocessing can improve effect size detection for skewed features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Interpreting Results by Cluster\n",
    "\n",
    "Let's examine what characterizes each cluster specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get characteristics for each cluster\n",
    "print(\"CLUSTER CHARACTERISTICS (Ranked by Importance Score)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    print(f\"\\nüîç CLUSTER {cluster_id} PROFILE:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cluster_chars = profiler.get_cluster_characteristics(\n",
    "        cluster_id=cluster_id, top_n=8, rank_by='importance_score'\n",
    "    )\n",
    "    \n",
    "    for _, row in cluster_chars.iterrows():\n",
    "        # Significance markers\n",
    "        if row['p_value_corrected'] < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif row['p_value_corrected'] < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif row['p_value_corrected'] < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        if row['effect_size_type'] == 'cohens_d':\n",
    "            if row['effect_size'] > 0.8:\n",
    "                effect_desc = \"Large\"\n",
    "            elif row['effect_size'] > 0.5:\n",
    "                effect_desc = \"Medium\"\n",
    "            elif row['effect_size'] > 0.2:\n",
    "                effect_desc = \"Small\"\n",
    "            else:\n",
    "                effect_desc = \"Negligible\"\n",
    "        else:  # Cram√©r's V\n",
    "            if row['effect_size'] > 0.5:\n",
    "                effect_desc = \"Large\"\n",
    "            elif row['effect_size'] > 0.3:\n",
    "                effect_desc = \"Medium\"\n",
    "            elif row['effect_size'] > 0.1:\n",
    "                effect_desc = \"Small\"\n",
    "            else:\n",
    "                effect_desc = \"Negligible\"\n",
    "        \n",
    "        print(f\"  {row['feature']:18} | Importance: {row['importance_score']:6.2f} | \"\n",
    "              f\"Effect: {effect_desc:8} ({row['effect_size']:.3f}) | p={row['p_value_corrected']:.4f} {sig}\")\n",
    "    \n",
    "    # Show actual values for top features\n",
    "    top_feature = cluster_chars.iloc[0]['feature']\n",
    "    cluster_data = data[data['cluster'] == cluster_id]\n",
    "    other_data = data[data['cluster'] != cluster_id]\n",
    "    \n",
    "    if top_feature in data.select_dtypes(include=[np.number]).columns:\n",
    "        cluster_mean = cluster_data[top_feature].mean()\n",
    "        other_mean = other_data[top_feature].mean()\n",
    "        print(f\"  üìä {top_feature}: Cluster avg = {cluster_mean:.1f}, Others avg = {other_mean:.1f}\")\n",
    "    else:\n",
    "        cluster_mode = cluster_data[top_feature].mode().iloc[0]\n",
    "        cluster_pct = (cluster_data[top_feature] == cluster_mode).mean() * 100\n",
    "        print(f\"  üìä {top_feature}: {cluster_pct:.1f}% are '{cluster_mode}' in this cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Understanding the Statistical Approach\n",
    "\n",
    "Let's explain what the profiler is doing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STATISTICAL METHODOLOGY EXPLAINED\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"üî¨ TESTS PERFORMED:\")\n",
    "print(\"  ‚Ä¢ Continuous features: Kolmogorov-Smirnov test\")\n",
    "print(\"    - Tests if distributions differ between cluster vs. rest\")\n",
    "print(\"    - Non-parametric (no assumptions about data distribution)\")\n",
    "print()\n",
    "print(\"  ‚Ä¢ Categorical features: Chi-square or Fisher's exact test\")\n",
    "print(\"    - Tests if category proportions differ between cluster vs. rest\")\n",
    "print(\"    - Fisher's exact used for small expected frequencies\")\n",
    "print()\n",
    "print(\"üìè EFFECT SIZES:\")\n",
    "print(\"  ‚Ä¢ Cohen's d (continuous): Standardized difference between means\")\n",
    "print(\"    - Small: 0.2, Medium: 0.5, Large: 0.8+\")\n",
    "print()\n",
    "print(\"  ‚Ä¢ Cram√©r's V (categorical): Strength of association\")\n",
    "print(\"    - Small: 0.1, Medium: 0.3, Large: 0.5+\")\n",
    "print()\n",
    "print(\"üîß MULTIPLE TESTING CORRECTION:\")\n",
    "print(\"  ‚Ä¢ Benjamini-Hochberg FDR correction\")\n",
    "print(\"  ‚Ä¢ Controls false discovery rate (more appropriate than Bonferroni)\")\n",
    "print()\n",
    "print(\"‚≠ê IMPORTANCE SCORE:\")\n",
    "print(\"  ‚Ä¢ Formula: effect_size √ó (-log10(corrected_p_value))\")\n",
    "print(\"  ‚Ä¢ Combines statistical significance with practical importance\")\n",
    "print(\"  ‚Ä¢ Higher scores = more important for cluster characterization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validation - Did We Get Expected Results?\n",
    "\n",
    "Let's check if our profiler correctly identified meaningful vs. random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by feature type\n",
    "print(\"VALIDATION: Expected vs. Actual Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group features by expected significance\n",
    "meaningful_features = ['income', 'age', 'education_years', 'experience', 'monthly_spending', 'job_category']\n",
    "random_features = ['random_category', 'random_normal', 'random_uniform']\n",
    "\n",
    "# Check how many meaningful features are significant\n",
    "meaningful_results = results_raw[results_raw['feature'].isin(meaningful_features)]\n",
    "meaningful_significant = meaningful_results['significant_corrected'].sum()\n",
    "meaningful_total = len(meaningful_results)\n",
    "\n",
    "# Check how many random features are significant (should be few)\n",
    "random_results = results_raw[results_raw['feature'].isin(random_features)]\n",
    "random_significant = random_results['significant_corrected'].sum()\n",
    "random_total = len(random_results)\n",
    "\n",
    "print(f\"‚úÖ MEANINGFUL FEATURES:\")\n",
    "print(f\"   Significant: {meaningful_significant}/{meaningful_total} ({meaningful_significant/meaningful_total*100:.1f}%)\")\n",
    "print(f\"   Expected: High percentage (these should differentiate clusters)\")\n",
    "print()\n",
    "print(f\"‚ùå RANDOM FEATURES:\")\n",
    "print(f\"   Significant: {random_significant}/{random_total} ({random_significant/random_total*100:.1f}%)\")\n",
    "print(f\"   Expected: ~5% (false discovery rate)\")\n",
    "print()\n",
    "\n",
    "# Show top meaningful vs random features\n",
    "print(\"Top Meaningful Features by Importance:\")\n",
    "top_meaningful = meaningful_results.nlargest(5, 'importance_score')\n",
    "for _, row in top_meaningful.iterrows():\n",
    "    print(f\"  {row['feature']:18} | Importance: {row['importance_score']:6.2f} | Cluster: {row['cluster']}\")\n",
    "\n",
    "print(\"\\nRandom Features (should have low importance):\")\n",
    "for _, row in random_results.nlargest(3, 'importance_score').iterrows():\n",
    "    print(f\"  {row['feature']:18} | Importance: {row['importance_score']:6.2f} | Cluster: {row['cluster']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ What Cluster Profiler Does Well:\n",
    "1. **Identifies meaningful features** that truly differentiate clusters\n",
    "2. **Controls false discoveries** through multiple testing correction\n",
    "3. **Measures effect sizes** to distinguish statistical vs. practical significance\n",
    "4. **Handles different data types** (continuous, categorical, skewed)\n",
    "5. **Ranks by importance** rather than just p-values\n",
    "\n",
    "### üéØ When to Use Cluster Profiler:\n",
    "- After clustering, to understand what makes each cluster unique\n",
    "- To validate that clusters are meaningful (not just statistical artifacts)\n",
    "- To select features for cluster interpretation or downstream analysis\n",
    "- To compare different clustering approaches\n",
    "\n",
    "### ‚ö†Ô∏è Important Considerations:\n",
    "- **Effect size matters more than p-values** for practical importance\n",
    "- **Preprocessing can help** with highly skewed features\n",
    "- **Multiple testing correction is crucial** when testing many features\n",
    "- **Sample size affects results** - larger samples detect smaller effects\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "Try the profiler on your own data and see what insights you discover!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}